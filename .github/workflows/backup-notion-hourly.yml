name: Notion API Backup (Auto-Discovery)

permissions:
  contents: write

on:
  schedule:
    - cron: '0 * * * *'  # Every hour
  workflow_dispatch:      # Manual trigger

jobs:
  backup:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          pip install notion-client pandas

      - name: Backup All Accessible Notion Databases
        env:
          NOTION_TOKEN: ${{ secrets.NOTION_TOKEN }}
        run: |
          python - <<'EOF'
          import os
          from notion_client import Client
          from pathlib import Path
          import json
          import pandas as pd

          # Initialize Client
          notion = Client(auth=os.environ["NOTION_TOKEN"])

          # Setup Output Directory
          output_dir = Path("notion-backup")
          output_dir.mkdir(exist_ok=True)

          print("--- Starting Auto-Discovery Backup ---")

          # 1. SEARCH for all accessible objects (No filter to avoid API validation errors)
          try:
              search_results = notion.search().get("results", [])
              # Manually filter for databases only
              databases = [obj for obj in search_results if obj.get("object") == "database"]
          except Exception as e:
              print(f"CRITICAL ERROR during search: {e}")
              exit(1)

          if not databases:
              print("No databases found! Make sure you have 'invited' the bot to your pages in Notion.")
          else:
              print(f"Found {len(databases)} databases.\n")

          for db in databases:
              db_id = db["id"]
              title_list = db.get("title", [])
              title = title_list[0].get("plain_text", f"Untitled_{db_id[:8]}") if title_list else f"Untitled_{db_id[:8]}"
              
              print(f"Processing: {title} ({db_id})")
              
              # Check for properties (the schema)
              properties = db.get("properties", {})
              if not properties:
                  print(f"  [!] Skipping: No properties visible. Check 'Connect to' in Notion.")
                  continue

              # 2. QUERY all rows (handling pagination)
              all_rows = []
              has_more = True
              next_cursor = None
              
              try:
                  while has_more:
                      query_result = notion.databases.query(
                          database_id=db_id, 
                          start_cursor=next_cursor,
                          page_size=100
                      )
                      all_rows.extend(query_result["results"])
                      has_more = query_result["has_more"]
                      next_cursor = query_result["next_cursor"]
                  
                  print(f"  Rows found: {len(all_rows)}")
              except Exception as e:
                  print(f"  [!] Error querying rows: {e}")
                  continue

              # 3. EXTRACT DATA logic
              data = []
              for row in all_rows:
                  props = row.get("properties", {})
                  row_data = {
                      "Page ID": row["id"],
                      "URL": row.get("url"),
                      "Created": row["created_time"],
                      "Edited": row["last_edited_time"]
                  }
                  
                  for prop_name, prop_content in props.items():
                      p_type = prop_content.get("type")
                      val_obj = prop_content.get(p_type)
                      
                      # Parsing different Notion types into strings
                      if p_type in ["title", "rich_text"]:
                          parsed_val = "".join([t.get("plain_text", "") for t in val_obj]) if val_obj else ""
                      elif p_type == "select" and val_obj:
                          parsed_val = val_obj.get("name", "")
                      elif p_type == "multi_select" and val_obj:
                          parsed_val = ", ".join([v.get("name", "") for v in val_obj])
                      elif p_type == "date" and val_obj:
                          parsed_val = val_obj.get("start", "")
                      elif p_type == "number":
                          parsed_val = val_obj
                      elif p_type == "checkbox":
                          parsed_val = val_obj
                      elif p_type == "url":
                          parsed_val = val_obj
                      elif p_type == "email":
                          parsed_val = val_obj
                      elif p_type == "phone_number":
                          parsed_val = val_obj
                      else:
                          parsed_val = "" # Skip complex types for basic CSV
                      
                      row_data[prop_name] = parsed_val
                  
                  data.append(row_data)

              # 4. SAVE TO CSV
              if data:
                  # Sanitize filename
                  safe_title = "".join(c if c.isalnum() or c in " -" else "_" for c in title).strip()
                  csv_path = output_dir / f"{safe_title}.csv"
                  
                  df = pd.DataFrame(data)
                  df.to_csv(csv_path, index=False)
                  print(f"  [SUCCESS] Saved to: {csv_path.name}")
              else:
                  print(f"  [?] Database is empty.")

          print("\n--- Backup script finished ---")
          EOF

      - name: Commit and push changes
        run: |
          git config user.name "Nihad-BIT"
          git config user.email "nihadNV@proton.me"
          git add notion-backup/
          # Only commit if there are changes
          git diff --staged --quiet || (git commit -m "Auto-backup Notion: $(date '+%Y-%m-%d %H:%M')" && git push)
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
