name: Notion API Backup (Deep Scan)

permissions:
  contents: write

on:
  schedule:
    - cron: '0 * * * *'
  workflow_dispatch:

jobs:
  backup:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          pip install notion-client pandas

      - name: Backup Notion Data
        env:
          NOTION_TOKEN: ${{ secrets.NOTION_TOKEN }}
        run: |
          python - <<'EOF'
          import os
          from notion_client import Client
          from pathlib import Path
          import pandas as pd

          notion = Client(auth=os.environ["NOTION_TOKEN"])
          output_dir = Path("notion-backup")
          output_dir.mkdir(exist_ok=True)

          def extract_properties(row):
              props = row.get("properties", {})
              row_data = {"Page ID": row["id"], "URL": row.get("url")}
              for name, content in props.items():
                  p_type = content.get("type")
                  val = content.get(p_type)
                  if p_type in ["title", "rich_text"]:
                      row_data[name] = "".join([t.get("plain_text", "") for t in val]) if val else ""
                  elif p_type == "select" and val:
                      row_data[name] = val.get("name", "")
                  elif p_type == "multi_select" and val:
                      row_data[name] = ", ".join([v.get("name", "") for v in val])
                  elif p_type == "date" and val:
                      row_data[name] = val.get("start", "")
                  else:
                      row_data[name] = str(val) if val is not None else ""
              return row_data

          def backup_database(db_id, title):
              print(f"  -> Extracting Database: {title}")
              all_rows = []
              has_more, cursor = True, None
              while has_more:
                  res = notion.databases.query(database_id=db_id, start_cursor=cursor)
                  all_rows.extend(res["results"])
                  has_more, cursor = res["has_more"], res["next_cursor"]
              
              if all_rows:
                  df = pd.DataFrame([extract_properties(r) for r in all_rows])
                  safe_title = "".join(c if c.isalnum() or c in " -" else "_" for c in title).strip()
                  df.to_csv(output_dir / f"{safe_title}.csv", index=False)
                  print(f"  [SUCCESS] Saved {len(all_rows)} rows.")

          print("--- Starting Deep Scan Backup ---")
          search_results = notion.search().get("results", [])

          for item in search_results:
              item_id = item["id"]
              item_obj = item.get("object")
              
              # Get Title
              title = "Untitled"
              if item_obj == "database":
                  title = item["title"][0]["plain_text"] if item["title"] else "Database"
                  backup_database(item_id, title)
              else:
                  # If it's a page or data_source, look for databases INSIDE it
                  try:
                      children = notion.blocks.children.list(block_id=item_id).get("results", [])
                      for child in children:
                          if child["type"] == "child_database":
                              db_title = child["child_database"]["title"]
                              backup_database(child["id"], db_title)
                  except Exception:
                      continue

          print("\n--- Backup script finished ---")
          EOF

      - name: Commit and push changes
        run: |
          git config user.name "Nihad-BIT"
          git config user.email "nihadNV@proton.me"
          git add notion-backup/
          git diff --staged --quiet || (git commit -m "Auto-backup Notion: $(date '+%Y-%m-%d %H:%M')" && git push)
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
