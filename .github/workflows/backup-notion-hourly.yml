name: Notion API Backup (Recursive Crawler)

permissions:
  contents: write

on:
  schedule:
    - cron: '0 * * * *'
  workflow_dispatch:

jobs:
  backup:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          pip install notion-client pandas

      - name: Backup Notion Data
        env:
          NOTION_TOKEN: ${{ secrets.NOTION_TOKEN }}
        run: |
          python - <<'EOF'
          import os
          from notion_client import Client
          from pathlib import Path
          import pandas as pd

          notion = Client(auth=os.environ["NOTION_TOKEN"])
          output_dir = Path("notion-backup")
          output_dir.mkdir(exist_ok=True)

          def extract_properties(row):
              props = row.get("properties", {})
              row_data = {"Page ID": row["id"], "URL": row.get("url")}
              for name, content in props.items():
                  p_type = content.get("type")
                  val = content.get(p_type)
                  if p_type in ["title", "rich_text"]:
                      row_data[name] = "".join([t.get("plain_text", "") for t in val]) if val else ""
                  elif p_type == "select" and val:
                      row_data[name] = val.get("name", "")
                  elif p_type == "multi_select" and val:
                      row_data[name] = ", ".join([v.get("name", "") for v in val])
                  elif p_type == "date" and val:
                      row_data[name] = val.get("start", "")
                  elif p_type == "number":
                      row_data[name] = val
                  else:
                      row_data[name] = str(val) if val is not None else ""
              return row_data

          def backup_database(db_id, title):
              print(f"  [Found Database] Extracting: {title} ({db_id})")
              all_rows = []
              has_more, cursor = True, None
              try:
                  while has_more:
                      res = notion.databases.query(database_id=db_id, start_cursor=cursor)
                      all_rows.extend(res["results"])
                      has_more, cursor = res["has_more"], res["next_cursor"]
                  
                  if all_rows:
                      df = pd.DataFrame([extract_properties(r) for r in all_rows])
                      safe_title = "".join(c if c.isalnum() or c in " -" else "_" for c in title).strip()
                      df.to_csv(output_dir / f"{safe_title}.csv", index=False)
                      print(f"    -> SUCCESS: Saved {len(all_rows)} rows.")
              except Exception as e:
                  print(f"    -> ERROR: Could not query database {db_id}: {e}")

          def scan_block(block_id):
              """Recursively search for child_database blocks"""
              try:
                  children = notion.blocks.children.list(block_id=block_id).get("results", [])
                  for child in children:
                      c_type = child["type"]
                      
                      # If it's a database, back it up
                      if c_type == "child_database":
                          db_title = child["child_database"]["title"]
                          backup_database(child["id"], db_title)
                      
                      # If it's a page or a toggle/column, look inside it
                      if child.get("has_children"):
                          scan_block(child["id"])
              except Exception as e:
                  pass

          print("--- Starting Recursive Crawler Backup ---")
          
          # First, try a standard search
          print("Checking direct database access...")
          search_results = notion.search().get("results", [])
          
          if not search_results:
              print("No direct access found via search. Switching to deep-scan...")
          
          for item in search_results:
              if item["object"] == "database":
                  title = item["title"][0]["plain_text"] if item["title"] else "Untitled"
                  backup_database(item["id"], title)
              else:
                  # Crawl pages found in search
                  scan_block(item["id"])

          print("\n--- Backup script finished ---")
          EOF

      - name: Commit and push changes
        run: |
          git config user.name "Nihad-BIT"
          git config user.email "nihadNV@proton.me"
          git add notion-backup/
          git diff --staged --quiet || (git commit -m "Auto-backup Notion: $(date '+%Y-%m-%d %H:%M')" && git push)
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
